{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jithendra727/Bank-Account-System/blob/main/Sinhala_BiLSTM_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKSPNHmyAoe3"
      },
      "source": [
        "# ğŸ™ï¸ Sinhala-English Speech-to-Text â€” Custom BiLSTM Model\n",
        "### Final Year Project | Train your own model from scratch on Kaggle data\n",
        "\n",
        "**What this notebook builds:**\n",
        "- âœ… MFCC feature extraction (NumPy only, no librosa) â€” **your code**\n",
        "- âœ… Custom Sinhala-English vocabulary (English + full Sinhala script) â€” **your code**\n",
        "- âœ… BiLSTM + CTC neural network â€” **your architecture**\n",
        "- âœ… Training on 4 Kaggle datasets (Sinhala + English) â€” **your pipeline**\n",
        "- âœ… Auto-saves to Google Drive (survives session restarts)\n",
        "- âœ… Live microphone transcription after training\n",
        "\n",
        "**Run order:** Cell 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7 â†’ 8 â†’ 9 â†’ 10 â†’ 11 â†’ 12 â†’ 13\n",
        "\n",
        "> âš¡ **Enable GPU first:** Runtime â†’ Change runtime type â†’ T4 GPU"
      ],
      "id": "kKSPNHmyAoe3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E6OjnzPAoe5",
        "outputId": "4b11b486-8c5b-4f11-d7c3-86c3bd0c5555"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 1 â€” Install Dependencies\n",
        "# Run ONCE per session. Takes ~2 minutes.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "!pip install torch torchaudio -q\n",
        "!pip install numpy scipy matplotlib pandas tqdm -q\n",
        "!pip uninstall -y kaggle 2>/dev/null\n",
        "!pip install -U kaggle -q\n",
        "!sudo apt-get install -y ffmpeg -q 2>/dev/null\n",
        "\n",
        "import subprocess\n",
        "v = subprocess.run(['ffmpeg','-version'], capture_output=True, text=True)\n",
        "print(\"âœ… ffmpeg:\", v.stdout.split('\\n')[0])\n",
        "\n",
        "import torch\n",
        "print(\"âœ… PyTorch:\", torch.__version__)\n",
        "print(\"âœ… CUDA available:\", torch.cuda.is_available())\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"âš ï¸  No GPU detected! Go to Runtime â†’ Change runtime type â†’ T4 GPU\")\n",
        "    print(\"   Training will be extremely slow on CPU.\")\n",
        "else:\n",
        "    print(\"âœ… GPU:\", torch.cuda.get_device_name(0))\n",
        "print(\"\\nâœ… All dependencies installed. Move to Cell 2.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: kaggle 1.7.4.5\n",
            "Uninstalling kaggle-1.7.4.5:\n",
            "  Successfully uninstalled kaggle-1.7.4.5\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hReading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "âœ… ffmpeg: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "âœ… PyTorch: 2.10.0+cu128\n",
            "âœ… CUDA available: True\n",
            "âœ… GPU: Tesla T4\n",
            "\n",
            "âœ… All dependencies installed. Move to Cell 2.\n"
          ]
        }
      ],
      "execution_count": 1,
      "id": "-E6OjnzPAoe5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x-llJMbAoe5",
        "outputId": "c7e840c8-6d3a-46c3-c88b-3ea645e0ed99"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 2 â€” Import Libraries\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import pandas as pd\n",
        "import os, json, time, subprocess, base64, shutil, random\n",
        "from datetime import datetime\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, Javascript, Audio, HTML, clear_output\n",
        "from google.colab import output, drive, files\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"âœ… Libraries imported\")\n",
        "print(f\"ğŸ’» Device: {DEVICE}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries imported\n",
            "ğŸ’» Device: cuda\n"
          ]
        }
      ],
      "execution_count": 2,
      "id": "3x-llJMbAoe5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQM3SY2hAoe6",
        "outputId": "589b3b2e-275e-4c06-8d79-0231532b168e"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 3 â€” Mount Google Drive\n",
        "# All model checkpoints + logs save here automatically.\n",
        "# If session crashes, you restart from the last checkpoint.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Project folder on your Drive\n",
        "DRIVE_DIR = '/content/drive/MyDrive/SinhalaSTT'\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "os.makedirs(f'{DRIVE_DIR}/checkpoints', exist_ok=True)\n",
        "os.makedirs(f'{DRIVE_DIR}/logs', exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Google Drive mounted\")\n",
        "print(f\"ğŸ“ Project folder: {DRIVE_DIR}\")\n",
        "print(f\"   Checkpoints â†’ {DRIVE_DIR}/checkpoints/\")\n",
        "print(f\"   Training logs â†’ {DRIVE_DIR}/logs/\")\n",
        "\n",
        "# Check for existing checkpoints to resume training\n",
        "existing = sorted([f for f in os.listdir(f'{DRIVE_DIR}/checkpoints') if f.endswith('.pth')])\n",
        "if existing:\n",
        "    print(f\"\\nâ™»ï¸  Found {len(existing)} existing checkpoint(s):\")\n",
        "    for f in existing[-3:]:\n",
        "        print(f\"   - {f}\")\n",
        "    print(\"   â†’ Training will resume from the latest checkpoint!\")\n",
        "else:\n",
        "    print(\"\\nğŸ“ No checkpoints found â€” will train from scratch.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Google Drive mounted\n",
            "ğŸ“ Project folder: /content/drive/MyDrive/SinhalaSTT\n",
            "   Checkpoints â†’ /content/drive/MyDrive/SinhalaSTT/checkpoints/\n",
            "   Training logs â†’ /content/drive/MyDrive/SinhalaSTT/logs/\n",
            "\n",
            "ğŸ“ No checkpoints found â€” will train from scratch.\n"
          ]
        }
      ],
      "execution_count": 4,
      "id": "PQM3SY2hAoe6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Myz6QsxfAoe6",
        "outputId": "67e412ee-139f-468c-ba67-fda1e86c03d8"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 4 â€” Kaggle API Setup\n",
        "#\n",
        "# IMPORTANT: Use YOUR NEW token (not the old exposed one).\n",
        "# Get it from: https://www.kaggle.com/settings â†’ API â†’ Create New Token\n",
        "#\n",
        "# HOW TO ADD IT SAFELY (don't paste in code!):\n",
        "#   1. Click the ğŸ”‘ key icon in the left sidebar (Colab Secrets)\n",
        "#   2. Add secret name:  KAGGLE_USERNAME  â†’ value: your_username\n",
        "#   3. Add secret name:  KAGGLE_KEY       â†’ value: your_new_token\n",
        "#   4. Toggle \"Notebook access\" ON for both\n",
        "#   Then uncomment the userdata lines below.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "# â”€â”€ Option A: Use Colab Secrets (RECOMMENDED â€” most secure) â”€â”€\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "    os.environ['KAGGLE_KEY']      = userdata.get('KAGGLE_KEY')\n",
        "    print(\"âœ… Kaggle credentials loaded from Colab Secrets\")\n",
        "except Exception:\n",
        "    print(\"âš ï¸  Secrets not found. Using Option B below.\")\n",
        "\n",
        "# â”€â”€ Option B: Direct (less secure â€” only use in private notebook) â”€â”€\n",
        "# Uncomment and fill in ONLY if Option A didn't work:\n",
        "# os.environ['KAGGLE_USERNAME'] = 'your_username_here'\n",
        "# os.environ['KAGGLE_KEY']      = 'your_new_token_here'\n",
        "\n",
        "# â”€â”€ Verify connection â”€â”€\n",
        "result = subprocess.run(['kaggle', 'datasets', 'list', '--max-size', '1'],\n",
        "                        capture_output=True, text=True)\n",
        "if 'Error' not in result.stderr and result.returncode == 0:\n",
        "    print(\"âœ… Kaggle API connected successfully!\")\n",
        "else:\n",
        "    print(\"âŒ Kaggle connection failed. Check your credentials.\")\n",
        "    print(\"   Error:\", result.stderr[:200])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸  Secrets not found. Using Option B below.\n",
            "âŒ Kaggle connection failed. Check your credentials.\n",
            "   Error: \n"
          ]
        }
      ],
      "execution_count": 5,
      "id": "Myz6QsxfAoe6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cLjdZVZAoe6",
        "outputId": "b87966f1-14d6-4caa-eed4-1b36a9a297ad"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 5 â€” MFCC Feature Extractor (FROM SCRATCH)\n",
        "# Pure NumPy. No librosa. Every step is your own math.\n",
        "#\n",
        "# Pipeline:\n",
        "#   1. Pre-emphasis    : y[n] = x[n] - 0.97Â·x[n-1]\n",
        "#   2. Framing         : 25ms windows, 10ms step\n",
        "#   3. Hamming window  : 0.54 - 0.46Â·cos(2Ï€n/(N-1))\n",
        "#   4. FFT             : (1/N)|FFT(x)|Â²\n",
        "#   5. Mel filterbank  : 40 triangular filters on Mel scale\n",
        "#   6. Log compression : log(mel + Îµ)\n",
        "#   7. DCT (MFCC)      : Î£ log_melÂ·cos(Ï€Â·kÂ·(2n+1)/2M)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "class MFCCExtractor:\n",
        "    \"\"\"MFCC feature extraction built entirely from scratch using NumPy.\"\"\"\n",
        "\n",
        "    def __init__(self, sr=16000, n_ceps=13, n_filters=40,\n",
        "                 frame_len=0.025, frame_step=0.010, nfft=512, pre_emph=0.97):\n",
        "        self.sr         = sr\n",
        "        self.n_ceps     = n_ceps\n",
        "        self.n_filters  = n_filters\n",
        "        self.fl_samples = int(round(frame_len  * sr))\n",
        "        self.fs_samples = int(round(frame_step * sr))\n",
        "        self.nfft       = nfft\n",
        "        self.pre_emph   = pre_emph\n",
        "        self._fbank     = self._build_filterbank()   # pre-compute once\n",
        "\n",
        "    # â”€â”€ 1. Pre-emphasis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    def _pre_emphasis(self, s):\n",
        "        return np.concatenate([[s[0]], s[1:] - self.pre_emph * s[:-1]])\n",
        "\n",
        "    # â”€â”€ 2. Framing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    def _frame(self, s):\n",
        "        n_frames = 1 + max(0, int(np.ceil((len(s) - self.fl_samples) / self.fs_samples)))\n",
        "        pad      = (n_frames - 1) * self.fs_samples + self.fl_samples - len(s)\n",
        "        s        = np.pad(s, (0, max(0, pad)))\n",
        "        rows     = np.tile(np.arange(self.fl_samples), (n_frames, 1))\n",
        "        cols     = np.tile(np.arange(0, n_frames * self.fs_samples, self.fs_samples),\n",
        "                           (self.fl_samples, 1)).T\n",
        "        return s[(rows + cols).astype(np.int32)]\n",
        "\n",
        "    # â”€â”€ 3. Hamming window â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    def _hamming(self, N):\n",
        "        n = np.arange(N)\n",
        "        return 0.54 - 0.46 * np.cos(2 * np.pi * n / (N - 1))\n",
        "\n",
        "    # â”€â”€ 4. Power spectrum â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    def _power(self, frames):\n",
        "        return (1.0 / self.nfft) * np.abs(np.fft.rfft(frames, self.nfft)) ** 2\n",
        "\n",
        "    # â”€â”€ 5. Mel filterbank (pre-computed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    @staticmethod\n",
        "    def _hz2mel(f): return 2595.0 * np.log10(1.0 + f / 700.0)\n",
        "    @staticmethod\n",
        "    def _mel2hz(m): return 700.0 * (10.0 ** (m / 2595.0) - 1.0)\n",
        "\n",
        "    def _build_filterbank(self):\n",
        "        lo, hi = self._hz2mel(0), self._hz2mel(self.sr / 2)\n",
        "        mels   = np.linspace(lo, hi, self.n_filters + 2)\n",
        "        bins   = np.floor((self.nfft + 1) * self._mel2hz(mels) / self.sr).astype(int)\n",
        "        fbank  = np.zeros((self.n_filters, self.nfft // 2 + 1))\n",
        "        for m in range(self.n_filters):\n",
        "            for k in range(bins[m],   bins[m+1]): fbank[m,k] = (k-bins[m])/(bins[m+1]-bins[m])\n",
        "            for k in range(bins[m+1], bins[m+2]): fbank[m,k] = (bins[m+2]-k)/(bins[m+2]-bins[m+1])\n",
        "        return fbank\n",
        "\n",
        "    # â”€â”€ 6+7. Log + DCT â†’ MFCC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    def _dct_mfcc(self, log_mel):\n",
        "        M    = log_mel.shape[1]\n",
        "        mfcc = np.zeros((log_mel.shape[0], self.n_ceps))\n",
        "        for k in range(self.n_ceps):\n",
        "            mfcc[:, k] = np.sum(\n",
        "                log_mel * np.cos(np.pi * k * (2 * np.arange(M) + 1) / (2 * M)),\n",
        "                axis=1)\n",
        "        return mfcc\n",
        "\n",
        "    def extract(self, signal, target_sr=None):\n",
        "        \"\"\"\n",
        "        Full pipeline: raw audio (float32 numpy) â†’ MFCC matrix (T Ã— n_ceps).\n",
        "        Handles any sample rate by resampling to self.sr first.\n",
        "        \"\"\"\n",
        "        # Resample if needed\n",
        "        if target_sr is not None and target_sr != self.sr:\n",
        "            ratio  = self.sr / target_sr\n",
        "            n_new  = int(len(signal) * ratio)\n",
        "            signal = np.interp(np.linspace(0, len(signal)-1, n_new),\n",
        "                               np.arange(len(signal)), signal)\n",
        "        signal   = signal.astype(np.float32)\n",
        "        emph     = self._pre_emphasis(signal)\n",
        "        frames   = self._frame(emph)\n",
        "        frames  *= self._hamming(self.fl_samples)\n",
        "        power    = self._power(frames)\n",
        "        mel      = np.dot(power, self._fbank.T)\n",
        "        mel      = np.where(mel == 0, np.finfo(float).eps, mel)\n",
        "        log_mel  = np.log(mel)\n",
        "        return self._dct_mfcc(log_mel)\n",
        "\n",
        "# â”€â”€ Quick verification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "mfcc_ex = MFCCExtractor(sr=16000)\n",
        "dummy   = np.random.randn(16000).astype(np.float32)\n",
        "out     = mfcc_ex.extract(dummy)\n",
        "\n",
        "print(\"â”\"*55)\n",
        "print(\"  MFCC EXTRACTOR â€” VERIFIED âœ…\")\n",
        "print(\"â”\"*55)\n",
        "print(f\"  Input  : 16,000 samples  (1 sec @ 16kHz)\")\n",
        "print(f\"  Output : {out.shape[0]} frames Ã— {out.shape[1]} coefficients\")\n",
        "print(f\"  Filterbank: {mfcc_ex.n_filters} Mel triangular filters\")\n",
        "print(f\"  Steps  : Pre-emphasis â†’ Frame â†’ Hamming â†’ FFT\")\n",
        "print(f\"           â†’ Mel-filter â†’ Log â†’ DCT\")\n",
        "print(\"â”\"*55)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  MFCC EXTRACTOR â€” VERIFIED âœ…\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  Input  : 16,000 samples  (1 sec @ 16kHz)\n",
            "  Output : 99 frames Ã— 13 coefficients\n",
            "  Filterbank: 40 Mel triangular filters\n",
            "  Steps  : Pre-emphasis â†’ Frame â†’ Hamming â†’ FFT\n",
            "           â†’ Mel-filter â†’ Log â†’ DCT\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
          ]
        }
      ],
      "execution_count": 6,
      "id": "4cLjdZVZAoe6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_ENW__0Aoe7",
        "outputId": "7a3ce367-f803-4907-a92f-6da02bb11685"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 6 â€” Bilingual Vocabulary\n",
        "# Covers full Sinhala Unicode script + English + diacritics.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "class BilingualVocab:\n",
        "    \"\"\"\n",
        "    Complete Sinhala-English character vocabulary for CTC training.\n",
        "\n",
        "    Includes:\n",
        "    - English: a-z, digits 0-9, apostrophe\n",
        "    - Sinhala: 18 vowels, 41 consonants, all diacritics (hal kirima,\n",
        "               vowel marks, conjunct signs)\n",
        "    - Special: space, <blank> (CTC), <unk>\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # â”€â”€ English â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        english = list(\"abcdefghijklmnopqrstuvwxyz0123456789'\")\n",
        "\n",
        "        # â”€â”€ Sinhala vowels (independent) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        si_vowels = [\n",
        "            'à¶…','à¶†','à¶‡','à¶ˆ','à¶‰','à¶Š','à¶‹','à¶Œ','à¶','à¶',\n",
        "            'à¶','à¶','à¶‘','à¶’','à¶“','à¶”','à¶•','à¶–'\n",
        "        ]\n",
        "\n",
        "        # â”€â”€ Sinhala consonants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        si_consonants = [\n",
        "            'à¶š','à¶›','à¶œ','à¶','à¶','à¶Ÿ','à¶ ','à¶¡','à¶¢','à¶£',\n",
        "            'à¶¤','à¶¥','à¶¦','à¶§','à¶¨','à¶©','à¶ª','à¶«','à¶¬','à¶­',\n",
        "            'à¶®','à¶¯','à¶°','à¶±','à¶³','à¶´','à¶µ','à¶¶','à¶·','à¶¸',\n",
        "            'à¶¹','à¶º','à¶»','à¶½','à·€','à·','à·‚','à·ƒ','à·„','à·…','à·†'\n",
        "        ]\n",
        "\n",
        "        # â”€â”€ Sinhala diacritics (vowel marks + hal kirima + special signs) â”€â”€â”€\n",
        "        si_diacritics = [\n",
        "            'à·','à·','à·‘','à·’','à·“','à·”','à·–','à·˜','à·™','à·š',\n",
        "            'à·›','à·œ','à·','à·','à·Ÿ','à¶‚','à¶ƒ','à·Š',\n",
        "            'à·²','à·³'\n",
        "        ]\n",
        "\n",
        "        # â”€â”€ Special tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        special = [' ', '<blank>', '<unk>']\n",
        "\n",
        "        # â”€â”€ Build combined vocab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        self.chars       = english + si_vowels + si_consonants + si_diacritics + special\n",
        "        self.c2i         = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.i2c         = {i: c for i, c in enumerate(self.chars)}\n",
        "        self.blank_idx   = self.c2i['<blank>']\n",
        "        self.unk_idx     = self.c2i['<unk>']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.chars)\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Text â†’ list of integer indices. Unknown chars â†’ <unk>.\"\"\"\n",
        "        result = []\n",
        "        for ch in text.lower():\n",
        "            result.append(self.c2i.get(ch, self.unk_idx))\n",
        "        return result\n",
        "\n",
        "    def decode(self, indices, remove_blanks=True):\n",
        "        \"\"\"Indices â†’ text string.\"\"\"\n",
        "        skip = {self.blank_idx} if remove_blanks else set()\n",
        "        return ''.join(self.i2c.get(i,'') for i in indices if i not in skip)\n",
        "\n",
        "    def ctc_decode(self, indices):\n",
        "        \"\"\"\n",
        "        Greedy CTC decode:\n",
        "        1. Remove consecutive duplicate tokens\n",
        "        2. Remove blank tokens\n",
        "        \"\"\"\n",
        "        decoded, prev = [], None\n",
        "        for idx in indices:\n",
        "            if idx != prev and idx != self.blank_idx:\n",
        "                decoded.append(idx)\n",
        "            prev = idx\n",
        "        return self.decode(decoded, remove_blanks=False)\n",
        "\n",
        "# â”€â”€ Build vocabulary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "vocab = BilingualVocab()\n",
        "\n",
        "print(\"â”\"*55)\n",
        "print(\"  BILINGUAL VOCABULARY LOADED\")\n",
        "print(\"â”\"*55)\n",
        "print(f\"  Total characters : {len(vocab)}\")\n",
        "print(f\"  English          : a-z, 0-9, apostrophe   = 37\")\n",
        "print(f\"  Sinhala vowels   : independent form        = 18\")\n",
        "print(f\"  Sinhala consonants                         = 41\")\n",
        "print(f\"  Sinhala diacritics (marks, hal, anusvara)  = 20\")\n",
        "print(f\"  Special (<blank>, <unk>, space)             = 3\")\n",
        "print(f\"  Blank index (CTC): {vocab.blank_idx}\")\n",
        "print()\n",
        "# Test encode/decode\n",
        "test = \"hello à¶šà·œà·„à·œà¶¸à¶¯\"\n",
        "enc = vocab.encode(test)\n",
        "dec = vocab.decode(enc)\n",
        "print(f\"  Encode test: '{test}'\")\n",
        "print(f\"  â†’ {enc[:8]}...\")\n",
        "print(f\"  â†’ Decoded back: '{dec}'\")\n",
        "print(\"â”\"*55)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  BILINGUAL VOCABULARY LOADED\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  Total characters : 119\n",
            "  English          : a-z, 0-9, apostrophe   = 37\n",
            "  Sinhala vowels   : independent form        = 18\n",
            "  Sinhala consonants                         = 41\n",
            "  Sinhala diacritics (marks, hal, anusvara)  = 20\n",
            "  Special (<blank>, <unk>, space)             = 3\n",
            "  Blank index (CTC): 117\n",
            "\n",
            "  Encode test: 'hello à¶šà·œà·„à·œà¶¸à¶¯'\n",
            "  â†’ [7, 4, 11, 11, 14, 116, 55, 107]...\n",
            "  â†’ Decoded back: 'hello à¶šà·œà·„à·œà¶¸à¶¯'\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
          ]
        }
      ],
      "execution_count": 7,
      "id": "h_ENW__0Aoe7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPw2NaGfAoe8",
        "outputId": "a4d6bc82-2593-4ef8-8534-8026c243ece9"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 7 FIX â€” Updated for newer PyTorch\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "class BiLSTM_CTC(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM model with CTC loss for speech recognition.\n",
        "    Designed for bilingual Sinhala-English character-level prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=13, hidden_size=512, num_layers=4,\n",
        "                 num_classes=None, dropout=0.3):\n",
        "        super().__init__()\n",
        "        assert num_classes is not None\n",
        "        self.input_norm = nn.BatchNorm1d(input_size)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
        "        self.dropout    = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x, input_lengths=None):\n",
        "        B, T, F = x.shape\n",
        "        x_norm = self.input_norm(x.reshape(B*T, F)).reshape(B, T, F)\n",
        "\n",
        "        if input_lengths is not None:\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(\n",
        "                x_norm, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "            lstm_out, _ = self.lstm(packed)\n",
        "            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        else:\n",
        "            lstm_out, _ = self.lstm(x_norm)\n",
        "\n",
        "        out = self.layer_norm(lstm_out)\n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc(out)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).permute(1, 0, 2)\n",
        "        return log_probs\n",
        "\n",
        "# â”€â”€ Initialise model, loss, optimiser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "model = BiLSTM_CTC(\n",
        "    input_size=13,\n",
        "    hidden_size=512,\n",
        "    num_layers=4,\n",
        "    num_classes=len(vocab),\n",
        "    dropout=0.3\n",
        ").to(DEVICE)\n",
        "\n",
        "criterion = nn.CTCLoss(blank=vocab.blank_idx, zero_infinity=True, reduction='mean')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "\n",
        "# Fixed: removed verbose=True (not supported in newer PyTorch)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3\n",
        ")\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"â”\"*55)\n",
        "print(\"  MODEL ARCHITECTURE\")\n",
        "print(\"â”\"*55)\n",
        "print(model)\n",
        "print()\n",
        "print(f\"  Total parameters: {n_params:,}\")\n",
        "print(f\"  Vocab size:       {len(vocab)}\")\n",
        "print(f\"  Device:           {DEVICE}\")\n",
        "print(f\"  Optimiser:        AdamW  lr=3e-4  wd=1e-5\")\n",
        "print(f\"  LR scheduler:     ReduceLROnPlateau (patience=3)\")\n",
        "print(\"â”\"*55)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  MODEL ARCHITECTURE\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "BiLSTM_CTC(\n",
            "  (input_norm): BatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lstm): LSTM(13, 512, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
            "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=119, bias=True)\n",
            ")\n",
            "\n",
            "  Total parameters: 21,181,585\n",
            "  Vocab size:       119\n",
            "  Device:           cuda\n",
            "  Optimiser:        AdamW  lr=3e-4  wd=1e-5\n",
            "  LR scheduler:     ReduceLROnPlateau (patience=3)\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
          ]
        }
      ],
      "execution_count": 10,
      "id": "cPw2NaGfAoe8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsKWrwP_Aoe8",
        "outputId": "12506337-198c-48bc-ddd4-84889a71aa54"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 8 â€” Download Kaggle Datasets\n",
        "#\n",
        "# Downloads all 4 datasets. Large ones (sinscribe 12GB)\n",
        "# take 15-30 mins on Colab. Keep the tab open.\n",
        "# Files stay on Colab disk until session ends.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "DATASETS = {\n",
        "    'sinscribe': {\n",
        "        'slug'   : 'seniruepasinghe/sinscribe-sinhala-stt',\n",
        "        'dest'   : '/content/data/sinscribe',\n",
        "        'size'   : '~12 GB',\n",
        "        'lang'   : 'Sinhala',\n",
        "        'samples': '161k'\n",
        "    },\n",
        "    'multispk': {\n",
        "        'slug'   : 'keshan/multi-speaket-tts-dataset-sinhala',\n",
        "        'dest'   : '/content/data/multispk',\n",
        "        'size'   : '~1.7 GB',\n",
        "        'lang'   : 'Sinhala',\n",
        "        'samples': 'thousands'\n",
        "    },\n",
        "    'voicemakers': {\n",
        "        'slug'   : 'safnask/sinhalatts-dataset-publication-by-voicemakers',\n",
        "        'dest'   : '/content/data/voicemakers',\n",
        "        'size'   : '~6 GB',\n",
        "        'lang'   : 'Sinhala',\n",
        "        'samples': '~4 speakers'\n",
        "    },\n",
        "    'ljspeech': {\n",
        "        'slug'   : 'mathurinache/the-lj-speech-dataset',\n",
        "        'dest'   : '/content/data/ljspeech',\n",
        "        'size'   : '~3 GB',\n",
        "        'lang'   : 'English',\n",
        "        'samples': '13,100'\n",
        "    }\n",
        "}\n",
        "\n",
        "# â”€â”€ Choose which to download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Comment out any you want to skip (e.g. skip sinscribe first to test faster)\n",
        "TO_DOWNLOAD = ['multispk', 'ljspeech', 'voicemakers', 'sinscribe']  # sinscribe last (biggest)\n",
        "\n",
        "print(\"â”\"*60)\n",
        "print(\"  KAGGLE DATASET DOWNLOAD\")\n",
        "print(\"â”\"*60)\n",
        "\n",
        "for key in TO_DOWNLOAD:\n",
        "    ds = DATASETS[key]\n",
        "    dest = ds['dest']\n",
        "\n",
        "    # Skip if already downloaded (saves time on re-runs)\n",
        "    if os.path.exists(dest) and len(os.listdir(dest)) > 0:\n",
        "        print(f\"âœ… {key:15} already downloaded â€” skipping\")\n",
        "        continue\n",
        "\n",
        "    os.makedirs(dest, exist_ok=True)\n",
        "    print(f\"\\nğŸ“¥ Downloading {key} ({ds['size']}, {ds['lang']}, {ds['samples']} samples)...\")\n",
        "    print(f\"   This may take a while for large datasets. Keep the tab open!\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    result = subprocess.run(\n",
        "        ['kaggle', 'datasets', 'download', '-d', ds['slug'],\n",
        "         '-p', dest, '--unzip', '-q'],\n",
        "        capture_output=True, text=True\n",
        "    )\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        # Count files downloaded\n",
        "        n_files = sum(len(files) for _, _, files in os.walk(dest))\n",
        "        total_mb = sum(\n",
        "            os.path.getsize(os.path.join(r,f))\n",
        "            for r,_,fs in os.walk(dest) for f in fs\n",
        "        ) / 1e6\n",
        "        print(f\"âœ… {key}: {n_files} files, {total_mb:.0f} MB in {elapsed:.0f}s\")\n",
        "    else:\n",
        "        print(f\"âŒ {key} failed: {result.stderr[:200]}\")\n",
        "\n",
        "print(\"\\nâ”\"*30)\n",
        "print(\"ğŸ“‚ Dataset directory overview:\")\n",
        "for key in TO_DOWNLOAD:\n",
        "    dest = DATASETS[key]['dest']\n",
        "    if os.path.exists(dest):\n",
        "        print(f\"\\n  [{key}] {dest}\")\n",
        "        for item in sorted(os.listdir(dest))[:8]:\n",
        "            full = os.path.join(dest, item)\n",
        "            size = os.path.getsize(full)/1e6 if os.path.isfile(full) else 0\n",
        "            tag = f'({size:.1f}MB)' if size > 0 else '(folder)'\n",
        "            print(f\"    {'ğŸ“„' if os.path.isfile(full) else 'ğŸ“'} {item} {tag}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  KAGGLE DATASET DOWNLOAD\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "\n",
            "ğŸ“¥ Downloading multispk (~1.7 GB, Sinhala, thousands samples)...\n",
            "   This may take a while for large datasets. Keep the tab open!\n",
            "âœ… multispk: 5367 files, 2392 MB in 43s\n",
            "\n",
            "ğŸ“¥ Downloading ljspeech (~3 GB, English, 13,100 samples)...\n",
            "   This may take a while for large datasets. Keep the tab open!\n",
            "âœ… ljspeech: 13102 files, 3801 MB in 77s\n",
            "\n",
            "ğŸ“¥ Downloading voicemakers (~6 GB, Sinhala, ~4 speakers samples)...\n",
            "   This may take a while for large datasets. Keep the tab open!\n",
            "âœ… voicemakers: 5580 files, 7352 MB in 150s\n",
            "\n",
            "ğŸ“¥ Downloading sinscribe (~12 GB, Sinhala, 161k samples)...\n",
            "   This may take a while for large datasets. Keep the tab open!\n",
            "âœ… sinscribe: 161297 files, 14506 MB in 297s\n",
            "\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "ğŸ“‚ Dataset directory overview:\n",
            "\n",
            "  [multispk] /content/data/multispk\n",
            "    ğŸ“„ file-mapping.json (1.2MB)\n",
            "    ğŸ“„ file_index.tsv (1.2MB)\n",
            "    ğŸ“„ pn_sin_01_00001.wav (0.2MB)\n",
            "    ğŸ“„ pn_sin_01_00002.wav (0.2MB)\n",
            "    ğŸ“„ pn_sin_01_00003.wav (0.2MB)\n",
            "    ğŸ“„ pn_sin_01_00004.wav (0.5MB)\n",
            "    ğŸ“„ pn_sin_01_00005.wav (0.2MB)\n",
            "    ğŸ“„ pn_sin_01_00006.wav (0.4MB)\n",
            "\n",
            "  [ljspeech] /content/data/ljspeech\n",
            "    ğŸ“ LJSpeech-1.1 (folder)\n",
            "\n",
            "  [voicemakers] /content/data/voicemakers\n",
            "    ğŸ“ Dinithi-44100 (folder)\n",
            "    ğŸ“ Harini-44100 (folder)\n",
            "    ğŸ“ Isuru-44100Hz (folder)\n",
            "    ğŸ“„ Note.md (0.0MB)\n",
            "    ğŸ“ Yasindu-44100 (folder)\n",
            "\n",
            "  [sinscribe] /content/data/sinscribe\n",
            "    ğŸ“ data (folder)\n",
            "    ğŸ“„ dataset.csv (16.7MB)\n"
          ]
        }
      ],
      "execution_count": 11,
      "id": "XsKWrwP_Aoe8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_9UTLUSAoe9",
        "outputId": "cb6548ce-a8bd-4a56-ba39-4c54557374fe"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 9 FIX â€” Dataset Parser (handles all 4 datasets)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "def find_audio_files(root, exts={'.wav','.mp3','.flac','.ogg'}):\n",
        "    \"\"\"Recursively find all audio files under root.\"\"\"\n",
        "    found = []\n",
        "    for dirpath, _, filenames in os.walk(root):\n",
        "        for f in filenames:\n",
        "            if os.path.splitext(f)[1].lower() in exts:\n",
        "                found.append(os.path.join(dirpath, f))\n",
        "    return sorted(found)\n",
        "\n",
        "\n",
        "# â”€â”€ Dataset-specific loaders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def load_ljspeech(root, max_samples=None):\n",
        "    \"\"\"LJ Speech: pipe-separated, no header, format: ID|transcript|normalized\"\"\"\n",
        "    csv_path = os.path.join(root, 'LJSpeech-1.1', 'metadata.csv')\n",
        "    audio_dir = os.path.join(root, 'LJSpeech-1.1', 'wavs')\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        return []\n",
        "\n",
        "    # Read with pipe separator and NO header\n",
        "    df = pd.read_csv(csv_path, sep='|', header=None,\n",
        "                     names=['id', 'transcript', 'normalized'])\n",
        "\n",
        "    if max_samples:\n",
        "        df = df.sample(min(len(df), max_samples), random_state=42)\n",
        "\n",
        "    entries = []\n",
        "    for _, row in df.iterrows():\n",
        "        wav_path = os.path.join(audio_dir, f\"{row['id']}.wav\")\n",
        "        if os.path.exists(wav_path):\n",
        "            entries.append({\n",
        "                'file': wav_path,\n",
        "                'text': row['transcript'],\n",
        "                'sr': 22050,  # LJSpeech is 22kHz\n",
        "                'lang': 'en'\n",
        "            })\n",
        "    return entries\n",
        "\n",
        "\n",
        "def load_sinscribe(root, max_samples=None):\n",
        "    \"\"\"sinscribe: CSV with audio in data/ subfolder\"\"\"\n",
        "    csv_path = os.path.join(root, 'dataset.csv')\n",
        "    audio_dir = os.path.join(root, 'data')\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        return []\n",
        "\n",
        "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "    if max_samples:\n",
        "        df = df.sample(min(len(df), max_samples), random_state=42)\n",
        "\n",
        "    # Common column names in sinscribe datasets\n",
        "    possible_audio_cols = ['path', 'file', 'audio_path', 'filename', 'wav']\n",
        "    possible_text_cols = ['text', 'transcript', 'sentence']\n",
        "\n",
        "    audio_col = next((c for c in df.columns if c.lower() in possible_audio_cols), df.columns[0])\n",
        "    text_col = next((c for c in df.columns if c.lower() in possible_text_cols), df.columns[-1])\n",
        "\n",
        "    entries = []\n",
        "    for _, row in df.iterrows():\n",
        "        text = str(row.get(text_col, '')).strip()\n",
        "        if not text or text == 'nan':\n",
        "            continue\n",
        "\n",
        "        audio_file = str(row.get(audio_col, ''))\n",
        "        # Try multiple path combinations\n",
        "        for attempt in [\n",
        "            os.path.join(audio_dir, audio_file),\n",
        "            os.path.join(audio_dir, os.path.basename(audio_file)),\n",
        "            os.path.join(root, audio_file),\n",
        "        ]:\n",
        "            if os.path.exists(attempt):\n",
        "                entries.append({\n",
        "                    'file': attempt,\n",
        "                    'text': text,\n",
        "                    'sr': 16000,\n",
        "                    'lang': 'si'\n",
        "                })\n",
        "                break\n",
        "\n",
        "    return entries\n",
        "\n",
        "\n",
        "def load_multispk(root, max_samples=None):\n",
        "    \"\"\"Multi-speaker TTS: TSV file with direct WAV references\"\"\"\n",
        "    tsv_path = os.path.join(root, 'file_index.tsv')\n",
        "\n",
        "    if not os.path.exists(tsv_path):\n",
        "        return []\n",
        "\n",
        "    df = pd.read_csv(tsv_path, sep='\\t', encoding='utf-8')\n",
        "\n",
        "    if max_samples:\n",
        "        df = df.sample(min(len(df), max_samples), random_state=42)\n",
        "\n",
        "    # Column detection\n",
        "    audio_col = next((c for c in df.columns if 'file' in c.lower() or 'path' in c.lower()), df.columns[0])\n",
        "    text_col = next((c for c in df.columns if 'text' in c.lower() or 'transcript' in c.lower()), df.columns[-1])\n",
        "\n",
        "    entries = []\n",
        "    for _, row in df.iterrows():\n",
        "        text = str(row.get(text_col, '')).strip()\n",
        "        if not text or text == 'nan':\n",
        "            continue\n",
        "\n",
        "        audio_file = str(row.get(audio_col, ''))\n",
        "        wav_path = os.path.join(root, os.path.basename(audio_file))\n",
        "\n",
        "        if os.path.exists(wav_path):\n",
        "            entries.append({\n",
        "                'file': wav_path,\n",
        "                'text': text,\n",
        "                'sr': 16000,\n",
        "                'lang': 'si'\n",
        "            })\n",
        "\n",
        "    return entries\n",
        "\n",
        "\n",
        "def load_voicemakers(root, max_samples=None):\n",
        "    \"\"\"VoiceMakers: Multiple speaker folders, each with metadata.csv\"\"\"\n",
        "    entries = []\n",
        "\n",
        "    # Find all speaker folders\n",
        "    speaker_folders = [\n",
        "        os.path.join(root, d) for d in os.listdir(root)\n",
        "        if os.path.isdir(os.path.join(root, d))\n",
        "    ]\n",
        "\n",
        "    for speaker_folder in speaker_folders:\n",
        "        # Each speaker has nested folder structure: Speaker/Speaker/metadata.csv\n",
        "        nested = os.path.join(speaker_folder, os.path.basename(speaker_folder))\n",
        "        csv_path = os.path.join(nested, 'metadata.csv')\n",
        "\n",
        "        if not os.path.exists(csv_path):\n",
        "            # Try direct metadata.csv\n",
        "            csv_path = os.path.join(speaker_folder, 'metadata.csv')\n",
        "\n",
        "        if not os.path.exists(csv_path):\n",
        "            continue\n",
        "\n",
        "        df = pd.read_csv(csv_path, sep='|', header=None,\n",
        "                         names=['id', 'transcript', 'normalized'])\n",
        "\n",
        "        # Audio files are in wavs/ subfolder\n",
        "        audio_dir = os.path.join(os.path.dirname(csv_path), 'wavs')\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            text = str(row['transcript']).strip()\n",
        "            if not text or text == 'nan':\n",
        "                continue\n",
        "\n",
        "            wav_path = os.path.join(audio_dir, f\"{row['id']}.wav\")\n",
        "            if os.path.exists(wav_path):\n",
        "                entries.append({\n",
        "                    'file': wav_path,\n",
        "                    'text': text,\n",
        "                    'sr': 44100,  # VoiceMakers is 44.1kHz\n",
        "                    'lang': 'si'\n",
        "                })\n",
        "\n",
        "    if max_samples and len(entries) > max_samples:\n",
        "        entries = random.sample(entries, max_samples)\n",
        "\n",
        "    return entries\n",
        "\n",
        "\n",
        "# â”€â”€ Load all datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "print(\"â”\"*60)\n",
        "print(\"  PARSING DATASETS\")\n",
        "print(\"â”\"*60)\n",
        "\n",
        "all_entries = []\n",
        "\n",
        "dataset_loaders = [\n",
        "    ('sinscribe',   '/content/data/sinscribe',   load_sinscribe,   50000),\n",
        "    ('ljspeech',    '/content/data/ljspeech',    load_ljspeech,    10000),\n",
        "    ('multispk',    '/content/data/multispk',    load_multispk,    None),\n",
        "    ('voicemakers', '/content/data/voicemakers', load_voicemakers, None),\n",
        "]\n",
        "\n",
        "for name, path, loader_func, max_s in dataset_loaders:\n",
        "    if not os.path.exists(path) or not os.listdir(path):\n",
        "        print(f\"  âš ï¸  {name}: not downloaded â€” skipping\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n  ğŸ“‚ Parsing [{name}]...\")\n",
        "    try:\n",
        "        entries = loader_func(path, max_samples=max_s)\n",
        "        all_entries.extend(entries)\n",
        "        print(f\"  âœ… {name}: {len(entries):,} entries loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ {name} failed: {e}\")\n",
        "\n",
        "# Shuffle combined data\n",
        "random.shuffle(all_entries)\n",
        "\n",
        "print(f\"\\nâ”\"*30)\n",
        "print(f\"  TOTAL ENTRIES: {len(all_entries):,}\")\n",
        "si_count = sum(1 for e in all_entries if e['lang']=='si')\n",
        "en_count = sum(1 for e in all_entries if e['lang']=='en')\n",
        "print(f\"  Sinhala: {si_count:,} | English: {en_count:,}\")\n",
        "\n",
        "if len(all_entries) > 0:\n",
        "    print(f\"\\n  ğŸ“ Sample entries:\")\n",
        "    for e in all_entries[:3]:\n",
        "        print(f\"    [{e['lang'].upper()}] {e['text'][:50]}...\")\n",
        "        print(f\"           â†’ {e['file']}\")\n",
        "else:\n",
        "    print(\"\\n  âŒ Still 0 entries. Run debug cell below:\")\n",
        "    print(\"     !ls -la /content/data/sinscribe/data/ | head -20\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  PARSING DATASETS\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "\n",
            "  ğŸ“‚ Parsing [sinscribe]...\n",
            "  âœ… sinscribe: 0 entries loaded\n",
            "\n",
            "  ğŸ“‚ Parsing [ljspeech]...\n",
            "  âœ… ljspeech: 10,000 entries loaded\n",
            "\n",
            "  ğŸ“‚ Parsing [multispk]...\n",
            "  âœ… multispk: 5,364 entries loaded\n",
            "\n",
            "  ğŸ“‚ Parsing [voicemakers]...\n",
            "  âœ… voicemakers: 5,000 entries loaded\n",
            "\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "â”\n",
            "  TOTAL ENTRIES: 20,364\n",
            "  Sinhala: 10,364 | English: 10,000\n",
            "\n",
            "  ğŸ“ Sample entries:\n",
            "    [SI] à·ƒà·‘à¶¸ à·…à¶¸à¶ºà·™à¶šà·Š à¶¸à·š à·€à·ƒà¶»à·š à¶‰à¶¯à·’à¶»à·’ à·à·Šâ€à¶»à·šà¶«à·’à¶œà¶­ à¶ à·’à¶­à·Šâ€à¶» à·à·’à¶½à·Šà¶´à·’à¶ºà·...\n",
            "           â†’ /content/data/voicemakers/Dinithi-44100/Dinithi-44100/wavs/DIN_060_15.wav\n",
            "    [SI] pn_sin_01_02177.wav...\n",
            "           â†’ /content/data/multispk/pn_sin_01_02177.wav\n",
            "    [SI] à¶¸à·š à¶±à·’à·ƒà· à¶šà·à¶§à·Šà¶§à·š à¶»à¶¢à¶­à·”à¶¸à· à¶‹à¶©à·”à¶œà¶¸à·Šà¶´à·… à·ƒà¶šà¶½à¶šà¶½à· à·€à¶½à·Šà¶½à¶· à¶»à¶¢à·” à¶¸à¶œ...\n",
            "           â†’ /content/data/voicemakers/Harini-44100/Harini-44100/wavs/HN_119_09.wav\n"
          ]
        }
      ],
      "execution_count": 17,
      "id": "6_9UTLUSAoe9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogEYbGniAoe9",
        "outputId": "5c5bbc13-eb54-47d5-a56e-fbd8f99bce0a"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 10 â€” PyTorch Dataset + DataLoader\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "def load_audio_robust(path, target_sr=16000):\n",
        "    \"\"\"\n",
        "    Load any audio file (wav/mp3/flac) as float32 numpy array.\n",
        "    Uses ffmpeg for non-wav formats.\n",
        "    Returns (audio_array, sample_rate) or (None, None) on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ext = os.path.splitext(path)[1].lower()\n",
        "\n",
        "        if ext == '.wav':\n",
        "            sr, audio = wavfile.read(path)\n",
        "            if audio.ndim > 1:          # stereo â†’ mono\n",
        "                audio = audio.mean(axis=1)\n",
        "            audio = audio.astype(np.float32)\n",
        "            if audio.max() > 1.0:\n",
        "                audio /= 32768.0        # int16 â†’ float32 normalise\n",
        "        else:\n",
        "            # Convert mp3/flac/ogg via ffmpeg\n",
        "            tmp = '/tmp/_audio_load.wav'\n",
        "            r = subprocess.run(\n",
        "                ['ffmpeg', '-y', '-i', path, '-ar', str(target_sr),\n",
        "                 '-ac', '1', '-f', 'wav', tmp],\n",
        "                capture_output=True\n",
        "            )\n",
        "            if r.returncode != 0:\n",
        "                return None, None\n",
        "            sr, audio = wavfile.read(tmp)\n",
        "            audio = audio.astype(np.float32) / 32768.0\n",
        "\n",
        "        # Resample if needed\n",
        "        if sr != target_sr:\n",
        "            ratio = target_sr / sr\n",
        "            n_new = int(len(audio) * ratio)\n",
        "            audio = np.interp(\n",
        "                np.linspace(0, len(audio)-1, n_new),\n",
        "                np.arange(len(audio)), audio\n",
        "            )\n",
        "            sr = target_sr\n",
        "\n",
        "        # Validate: must be at least 0.1s and at most 20s\n",
        "        if len(audio) < 0.1 * sr or len(audio) > 20 * sr:\n",
        "            return None, None\n",
        "\n",
        "        return audio.astype(np.float32), sr\n",
        "\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "class SpeechDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset that:\n",
        "    1. Loads audio files from disk\n",
        "    2. Runs MFCC extraction (from scratch)\n",
        "    3. Encodes transcript text to integer labels\n",
        "    4. Filters out bad files on the fly\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, entries, vocab, mfcc_extractor, max_label_len=200):\n",
        "        self.entries   = entries\n",
        "        self.vocab     = vocab\n",
        "        self.mfcc_ex   = mfcc_extractor\n",
        "        self.max_label = max_label_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.entries[idx]\n",
        "        audio, sr = load_audio_robust(entry['file'])\n",
        "\n",
        "        if audio is None:\n",
        "            return None   # collate_fn will filter this\n",
        "\n",
        "        mfcc  = self.mfcc_ex.extract(audio)                      # (T, 13)\n",
        "        label = self.vocab.encode(entry['text'])[:self.max_label] # list of ints\n",
        "\n",
        "        if len(mfcc) < 5 or len(label) < 1:\n",
        "            return None\n",
        "\n",
        "        return (\n",
        "            torch.FloatTensor(mfcc),        # (T, 13)\n",
        "            torch.LongTensor(label),         # (L,)\n",
        "            mfcc.shape[0],                   # input_len\n",
        "            len(label)                        # target_len\n",
        "        )\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Pad variable-length sequences into fixed-size batch tensors.\"\"\"\n",
        "    # Filter out failed samples (None)\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if not batch:\n",
        "        return None\n",
        "\n",
        "    mfccs, labels, in_lens, tgt_lens = zip(*batch)\n",
        "\n",
        "    # Pad MFCCs to max length in batch\n",
        "    max_t   = max(m.shape[0] for m in mfccs)\n",
        "    padded  = torch.zeros(len(batch), max_t, mfccs[0].shape[1])\n",
        "    for i, m in enumerate(mfccs):\n",
        "        padded[i, :m.shape[0]] = m\n",
        "\n",
        "    # Concatenate all labels (CTC expects 1D target tensor)\n",
        "    flat_labels = torch.cat([l for l in labels])\n",
        "\n",
        "    return (\n",
        "        padded,                              # (B, T_max, 13)\n",
        "        flat_labels,                         # (sum of label lengths,)\n",
        "        torch.IntTensor(in_lens),            # (B,)\n",
        "        torch.IntTensor(tgt_lens)            # (B,)\n",
        "    )\n",
        "\n",
        "\n",
        "# â”€â”€ Create train/validation split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "random.shuffle(all_entries)\n",
        "split       = int(0.95 * len(all_entries))\n",
        "train_data  = all_entries[:split]\n",
        "val_data    = all_entries[split:]\n",
        "\n",
        "train_ds = SpeechDataset(train_data, vocab, mfcc_ex)\n",
        "val_ds   = SpeechDataset(val_data,   vocab, mfcc_ex)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,\n",
        "                          collate_fn=collate_fn, num_workers=2,\n",
        "                          pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False,\n",
        "                          collate_fn=collate_fn, num_workers=2,\n",
        "                          pin_memory=True)\n",
        "\n",
        "print(\"â”\"*55)\n",
        "print(\"  DATASET READY\")\n",
        "print(\"â”\"*55)\n",
        "print(f\"  Train samples  : {len(train_data):,}\")\n",
        "print(f\"  Val samples    : {len(val_data):,}\")\n",
        "print(f\"  Batch size     : 8\")\n",
        "print(f\"  Train batches  : {len(train_loader):,}\")\n",
        "print(f\"  Val batches    : {len(val_loader):,}\")\n",
        "print(\"â”\"*55)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  DATASET READY\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  Train samples  : 19,345\n",
            "  Val samples    : 1,019\n",
            "  Batch size     : 8\n",
            "  Train batches  : 2,419\n",
            "  Val batches    : 128\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
          ]
        }
      ],
      "execution_count": 18,
      "id": "ogEYbGniAoe9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTaL_obdAoe-",
        "outputId": "99a8d332-8fc1-4ebd-c056-45eb4b91eea1"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 11 â€” Evaluation Metric: Word Error Rate\n",
        "# Implemented from scratch using dynamic programming.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "def word_error_rate(ref, hyp):\n",
        "    \"\"\"\n",
        "    Compute Word Error Rate (WER) using Wagner-Fischer algorithm.\n",
        "\n",
        "    WER = (S + D + I) / N\n",
        "    S = substitutions, D = deletions, I = insertions, N = ref words\n",
        "\n",
        "    Implemented from scratch with dynamic programming â€” O(nÂ·m) time.\n",
        "    \"\"\"\n",
        "    r = ref.lower().strip().split()\n",
        "    h = hyp.lower().strip().split()\n",
        "    R, H = len(r), len(h)\n",
        "\n",
        "    # DP table: dp[i][j] = min edits to align r[:i] with h[:j]\n",
        "    dp = np.zeros((R+1, H+1), dtype=int)\n",
        "    dp[0, :] = np.arange(H+1)\n",
        "    dp[:, 0] = np.arange(R+1)\n",
        "\n",
        "    for i in range(1, R+1):\n",
        "        for j in range(1, H+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                dp[i,j] = dp[i-1,j-1]\n",
        "            else:\n",
        "                dp[i,j] = 1 + min(dp[i-1,j], dp[i,j-1], dp[i-1,j-1])\n",
        "\n",
        "    edits = dp[R, H]\n",
        "    return (edits / max(R, 1)) * 100, edits, R\n",
        "\n",
        "def batch_wer(refs, hyps):\n",
        "    \"\"\"Average WER across a list of reference/hypothesis pairs.\"\"\"\n",
        "    total_wer, total_edits, total_words = 0, 0, 0\n",
        "    for r, h in zip(refs, hyps):\n",
        "        w, e, n = word_error_rate(r, h)\n",
        "        total_edits += e\n",
        "        total_words += n\n",
        "    return (total_edits / max(total_words, 1)) * 100\n",
        "\n",
        "# â”€â”€ Quick test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tests = [\n",
        "    (\"hello how are you\",   \"hello how are you\"),\n",
        "    (\"good morning everyone\", \"good morning everyoane\"),\n",
        "    (\"the project is done\",   \"project done\"),\n",
        "]\n",
        "print(\"â”\"*55)\n",
        "print(\"  WER METRIC â€” VERIFIED\")\n",
        "print(\"â”\"*55)\n",
        "for ref, hyp in tests:\n",
        "    w, e, n = word_error_rate(ref, hyp)\n",
        "    print(f\"  REF: {ref}\")\n",
        "    print(f\"  HYP: {hyp}\")\n",
        "    print(f\"  WER: {w:.1f}%  ({e} edits / {n} words)\")\n",
        "    print()\n",
        "print(\"â”\"*55)\n",
        "print(\"  âœ… WER evaluator ready (DP from scratch)\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  WER METRIC â€” VERIFIED\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  REF: hello how are you\n",
            "  HYP: hello how are you\n",
            "  WER: 0.0%  (0 edits / 4 words)\n",
            "\n",
            "  REF: good morning everyone\n",
            "  HYP: good morning everyoane\n",
            "  WER: 33.3%  (1 edits / 3 words)\n",
            "\n",
            "  REF: the project is done\n",
            "  HYP: project done\n",
            "  WER: 50.0%  (2 edits / 4 words)\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  âœ… WER evaluator ready (DP from scratch)\n"
          ]
        }
      ],
      "execution_count": 19,
      "id": "dTaL_obdAoe-"
    },
    {
      "cell_type": "code",
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 7 FIX v2 â€” Variable name conflict resolved\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "class BiLSTM_CTC(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM model with CTC loss for speech recognition.\n",
        "    Designed for bilingual Sinhala-English character-level prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=13, hidden_size=512, num_layers=4,\n",
        "                 num_classes=None, dropout=0.3):\n",
        "        super().__init__()\n",
        "        assert num_classes is not None\n",
        "        self.input_norm = nn.BatchNorm1d(input_size)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
        "        self.dropout    = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x, input_lengths=None):\n",
        "        # FIXED: renamed F to n_feats to avoid shadowing torch.nn.functional.F\n",
        "        B, T, n_feats = x.shape\n",
        "        x_norm = self.input_norm(x.reshape(B*T, n_feats)).reshape(B, T, n_feats)\n",
        "\n",
        "        if input_lengths is not None:\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(\n",
        "                x_norm, input_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "            lstm_out, _ = self.lstm(packed)\n",
        "            lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        else:\n",
        "            lstm_out, _ = self.lstm(x_norm)\n",
        "\n",
        "        out = self.layer_norm(lstm_out)\n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc(out)\n",
        "        log_probs = F.log_softmax(logits, dim=-1).permute(1, 0, 2)\n",
        "        return log_probs\n",
        "\n",
        "# â”€â”€ Re-initialize model with fix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "model = BiLSTM_CTC(\n",
        "    input_size=13,\n",
        "    hidden_size=512,\n",
        "    num_layers=4,\n",
        "    num_classes=len(vocab),\n",
        "    dropout=0.3\n",
        ").to(DEVICE)\n",
        "\n",
        "criterion = nn.CTCLoss(blank=vocab.blank_idx, zero_infinity=True, reduction='mean')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=3\n",
        ")\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"â”\"*55)\n",
        "print(\"  MODEL RE-INITIALIZED (bug fixed)\")\n",
        "print(\"â”\"*55)\n",
        "print(f\"  Total parameters: {n_params:,}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(\"â”\"*55)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtCULqPqI5II",
        "outputId": "5f157458-0a5f-42ff-fd91-bbe06d8b8dbd"
      },
      "id": "XtCULqPqI5II",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  MODEL RE-INITIALIZED (bug fixed)\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  Total parameters: 21,181,585\n",
            "  Device: cuda\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFWZwHqUAoe-",
        "outputId": "a63f2b36-f31c-4e3d-8642-a1300df09f65"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 12 â€” Training Loop\n",
        "#\n",
        "# Features:\n",
        "#   âœ… Saves checkpoint to Google Drive every epoch\n",
        "#   âœ… Resumes automatically from latest checkpoint\n",
        "#   âœ… Plots training + validation loss live\n",
        "#   âœ… Computes WER on validation set each epoch\n",
        "#   âœ… Gradient clipping (prevents exploding gradients)\n",
        "#   âœ… LR scheduler (reduces LR when loss plateaus)\n",
        "#   âœ… Keeps best model separately as best_model.pth\n",
        "#\n",
        "# Expected training times on T4 GPU:\n",
        "#   ~10k samples  : ~20 mins / epoch\n",
        "#   ~50k samples  : ~2 hrs / epoch  (use fewer epochs)\n",
        "#   ~160k samples : ~5 hrs / epoch  (use 3-5 epochs max)\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "EPOCHS      = 15     # Reduce to 5 if dataset > 50k samples\n",
        "GRAD_CLIP   = 5.0    # Max gradient norm (prevents exploding)\n",
        "BEST_WER    = float('inf')\n",
        "START_EPOCH = 0\n",
        "\n",
        "CKPT_DIR    = f'{DRIVE_DIR}/checkpoints'\n",
        "LOG_PATH    = f'{DRIVE_DIR}/logs/training_log.json'\n",
        "\n",
        "# â”€â”€ Resume from checkpoint if it exists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "existing_ckpts = sorted([f for f in os.listdir(CKPT_DIR) if f.startswith('epoch_')])\n",
        "if existing_ckpts:\n",
        "    latest = os.path.join(CKPT_DIR, existing_ckpts[-1])\n",
        "    ckpt   = torch.load(latest, map_location=DEVICE)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    START_EPOCH = ckpt['epoch'] + 1\n",
        "    BEST_WER    = ckpt.get('best_wer', float('inf'))\n",
        "    print(f\"â™»ï¸  Resumed from checkpoint: {existing_ckpts[-1]}\")\n",
        "    print(f\"   Starting at epoch {START_EPOCH+1}, best WER so far: {BEST_WER:.1f}%\")\n",
        "else:\n",
        "    print(\"ğŸ“ Starting training from scratch\")\n",
        "\n",
        "# â”€â”€ Logging setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "train_losses, val_losses, val_wers = [], [], []\n",
        "if os.path.exists(LOG_PATH):\n",
        "    with open(LOG_PATH) as f:\n",
        "        log = json.load(f)\n",
        "        train_losses = log.get('train_loss', [])\n",
        "        val_losses   = log.get('val_loss',   [])\n",
        "        val_wers     = log.get('val_wer',    [])\n",
        "\n",
        "\n",
        "def run_epoch(loader, training=True):\n",
        "    \"\"\"Run one full pass through the dataset.\"\"\"\n",
        "    model.train() if training else model.eval()\n",
        "    total_loss, n_batches = 0.0, 0\n",
        "\n",
        "    context = torch.enable_grad() if training else torch.no_grad()\n",
        "    with context:\n",
        "        for batch in tqdm(loader, desc='Train' if training else 'Val ',\n",
        "                          leave=False, ncols=80):\n",
        "            if batch is None:\n",
        "                continue\n",
        "            mfccs, labels, in_lens, tgt_lens = batch\n",
        "            mfccs  = mfccs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            log_probs = model(mfccs, in_lens)   # (T, B, C)\n",
        "\n",
        "            # CTC loss\n",
        "            loss = criterion(log_probs, labels, in_lens, tgt_lens)\n",
        "\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                continue\n",
        "\n",
        "            if training:\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "                optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            n_batches  += 1\n",
        "\n",
        "    return total_loss / max(n_batches, 1)\n",
        "\n",
        "\n",
        "def evaluate_wer(loader, n_samples=50):\n",
        "    \"\"\"Decode predictions and compute WER on a sample of the val set.\"\"\"\n",
        "    model.eval()\n",
        "    refs, hyps = [], []\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if batch is None or count >= n_samples:\n",
        "                break\n",
        "            mfccs, labels, in_lens, tgt_lens = batch\n",
        "            mfccs = mfccs.to(DEVICE)\n",
        "\n",
        "            log_probs = model(mfccs)                    # (T, B, C)\n",
        "            preds     = log_probs.argmax(dim=2)         # (T, B)\n",
        "            preds     = preds.permute(1, 0).cpu().numpy()  # (B, T)\n",
        "\n",
        "            # Reconstruct reference labels from flattened tensor\n",
        "            label_list = []\n",
        "            offset = 0\n",
        "            for tl in tgt_lens:\n",
        "                label_list.append(labels[offset:offset+tl].tolist())\n",
        "                offset += tl\n",
        "\n",
        "            for i in range(len(label_list)):\n",
        "                ref_text = vocab.decode(label_list[i])\n",
        "                hyp_text = vocab.ctc_decode(preds[i].tolist())\n",
        "                refs.append(ref_text)\n",
        "                hyps.append(hyp_text)\n",
        "                count += 1\n",
        "\n",
        "    return batch_wer(refs, hyps), refs[:3], hyps[:3]\n",
        "\n",
        "\n",
        "# â”€â”€ Main training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"â”\"*60)\n",
        "print(f\"  TRAINING â€” {EPOCHS} epochs  |  {len(train_loader)} batches/epoch\")\n",
        "print(f\"  Dataset: {len(train_data):,} train + {len(val_data):,} val samples\")\n",
        "print(f\"  Checkpoints â†’ {CKPT_DIR}\")\n",
        "print(\"â”\"*60)\n",
        "\n",
        "for epoch in range(START_EPOCH, EPOCHS):\n",
        "    t_start = time.time()\n",
        "\n",
        "    # â”€â”€ Train â”€â”€\n",
        "    train_loss = run_epoch(train_loader, training=True)\n",
        "\n",
        "    # â”€â”€ Validate â”€â”€\n",
        "    val_loss = run_epoch(val_loader, training=False)\n",
        "    wer, sample_refs, sample_hyps = evaluate_wer(val_loader)\n",
        "\n",
        "    # â”€â”€ LR step â”€â”€\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # â”€â”€ Record â”€â”€\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_wers.append(wer)\n",
        "\n",
        "    elapsed = time.time() - t_start\n",
        "    lr_now  = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1:02d}/{EPOCHS}  [{elapsed:.0f}s]\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Loss: {val_loss:.4f}  |  WER: {wer:.1f}%  |  LR: {lr_now:.2e}\")\n",
        "    print(f\"  Sample â€” REF: {sample_refs[0][:60] if sample_refs else 'â€”'}\")\n",
        "    print(f\"         â€” HYP: {sample_hyps[0][:60] if sample_hyps else 'â€”'}\")\n",
        "\n",
        "    # â”€â”€ Save checkpoint to Drive â”€â”€\n",
        "    ckpt_path = f'{CKPT_DIR}/epoch_{epoch+1:03d}_wer{wer:.1f}.pth'\n",
        "    torch.save({\n",
        "        'epoch'    : epoch,\n",
        "        'model'    : model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss'  : val_loss,\n",
        "        'wer'       : wer,\n",
        "        'best_wer'  : BEST_WER,\n",
        "        'vocab_size': len(vocab),\n",
        "    }, ckpt_path)\n",
        "    print(f\"  ğŸ’¾ Checkpoint saved â†’ Drive\")\n",
        "\n",
        "    # â”€â”€ Keep best model â”€â”€\n",
        "    if wer < BEST_WER:\n",
        "        BEST_WER = wer\n",
        "        shutil.copy(ckpt_path, f'{DRIVE_DIR}/best_model.pth')\n",
        "        print(f\"  ğŸ† New best model! WER: {wer:.1f}%\")\n",
        "\n",
        "    # â”€â”€ Save log â”€â”€\n",
        "    with open(LOG_PATH, 'w') as f:\n",
        "        json.dump({'train_loss': train_losses, 'val_loss': val_losses,\n",
        "                   'val_wer': val_wers}, f)\n",
        "\n",
        "    # â”€â”€ Live loss plot â”€â”€\n",
        "    if len(train_losses) >= 2:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4))\n",
        "        fig.patch.set_facecolor('#0f172a')\n",
        "        for ax in (ax1, ax2):\n",
        "            ax.set_facecolor('#1e293b')\n",
        "            ax.tick_params(colors='#94a3b8')\n",
        "            ax.spines[:].set_color('#334155')\n",
        "\n",
        "        ax1.plot(train_losses, label='Train', color='#3b82f6', lw=2)\n",
        "        ax1.plot(val_losses,   label='Val',   color='#f59e0b', lw=2)\n",
        "        ax1.set_title('CTC Loss', color='white', fontweight='bold')\n",
        "        ax1.set_xlabel('Epoch', color='#94a3b8')\n",
        "        ax1.legend()\n",
        "        ax1.grid(alpha=0.2)\n",
        "\n",
        "        ax2.plot(val_wers, color='#10b981', lw=2, marker='o', markersize=4)\n",
        "        ax2.set_title('Validation WER (%)', color='white', fontweight='bold')\n",
        "        ax2.set_xlabel('Epoch', color='#94a3b8')\n",
        "        ax2.set_ylabel('WER %', color='#94a3b8')\n",
        "        ax2.axhline(y=BEST_WER, color='#ef4444', ls='--', lw=1, label=f'Best: {BEST_WER:.1f}%')\n",
        "        ax2.legend(); ax2.grid(alpha=0.2)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        clear_output(wait=True)\n",
        "\n",
        "print(f\"\\n{'â”'*60}\")\n",
        "print(f\"  TRAINING COMPLETE\")\n",
        "print(f\"  Best WER: {BEST_WER:.1f}%\")\n",
        "print(f\"  Best model: {DRIVE_DIR}/best_model.pth\")\n",
        "print(f\"{'â”'*60}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Starting training from scratch\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "  TRAINING â€” 15 epochs  |  2419 batches/epoch\n",
            "  Dataset: 19,345 train + 1,019 val samples\n",
            "  Checkpoints â†’ /content/drive/MyDrive/SinhalaSTT/checkpoints\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|                                           | 0/2419 [00:00<?, ?it/s]/tmp/ipython-input-1737617612.py:15: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  sr, audio = wavfile.read(path)\n",
            "/tmp/ipython-input-1737617612.py:15: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  sr, audio = wavfile.read(path)\n",
            "Val :   0%|                                             | 0/128 [00:00<?, ?it/s]/tmp/ipython-input-1737617612.py:15: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  sr, audio = wavfile.read(path)\n",
            "/tmp/ipython-input-1737617612.py:15: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  sr, audio = wavfile.read(path)\n",
            "/tmp/ipython-input-1737617612.py:15: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  sr, audio = wavfile.read(path)\n",
            "/tmp/ipython-input-1737617612.py:15: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  sr, audio = wavfile.read(path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 01/15  [1669s]\n",
            "  Train Loss: 2.9762  |  Val Loss: 1.2856  |  WER: 91.8%  |  LR: 3.00e-04\n",
            "  Sample â€” REF: sin<unk>4191<unk>5072534842<unk>wav\n",
            "         â€” HYP: sin<unk>6<unk>wav\n",
            "  ğŸ’¾ Checkpoint saved â†’ Drive\n",
            "  ğŸ† New best model! WER: 91.8%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain:   0%|                                           | 0/2419 [00:00<?, ?it/s]/tmp/ipython-input-1737617612.py:15: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  sr, audio = wavfile.read(path)\n",
            "/tmp/ipython-input-1737617612.py:15: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
            "  sr, audio = wavfile.read(path)\n",
            "Train:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2054/2419 [22:57<03:25,  1.78it/s]"
          ]
        }
      ],
      "execution_count": null,
      "id": "gFWZwHqUAoe-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFeg4X1zAoe_"
      },
      "source": [
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "# CELL 13 â€” Live Microphone Transcription\n",
        "#\n",
        "# Run AFTER training. Loads the best model and transcribes\n",
        "# your microphone input continuously until you click STOP.\n",
        "# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "\n",
        "# â”€â”€ Load best trained model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "best_path = f'{DRIVE_DIR}/best_model.pth'\n",
        "if os.path.exists(best_path):\n",
        "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    print(f\"âœ… Loaded best model (WER: {ckpt.get('wer', '?'):.1f}%)\")\n",
        "else:\n",
        "    print(\"âš ï¸  No best_model.pth found. Use model from training (Cell 12).\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# â”€â”€ Audio capture bridge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def record_chunk(seconds=5, sr=16000):\n",
        "    js = Javascript(f\"\"\"\n",
        "    async function rec() {{\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({{\n",
        "            audio: {{ sampleRate: 16000, channelCount: 1,\n",
        "                      echoCancellation: true, noiseSuppression: true }}\n",
        "        }});\n",
        "        const mr = new MediaRecorder(stream, {{ mimeType: 'audio/webm;codecs=opus' }});\n",
        "        let chunks = [];\n",
        "        mr.ondataavailable = e => chunks.push(e.data);\n",
        "        mr.start();\n",
        "        await new Promise(r => setTimeout(r, {seconds * 1000}));\n",
        "        mr.stop();\n",
        "        return new Promise(r => {{\n",
        "            mr.onstop = async () => {{\n",
        "                const b = new Blob(chunks, {{ type: 'audio/webm' }});\n",
        "                const rd = new FileReader();\n",
        "                rd.readAsDataURL(b);\n",
        "                rd.onloadend = () => {{ stream.getTracks().forEach(t => t.stop()); r(rd.result); }};\n",
        "            }};\n",
        "        }});\n",
        "    }}\n",
        "    \"\"\")\n",
        "    display(js)\n",
        "    data = output.eval_js('rec()')\n",
        "    if not data: return None, None\n",
        "    raw  = base64.b64decode(data.split(',')[1])\n",
        "    with open('/tmp/live.webm','wb') as f: f.write(raw)\n",
        "    subprocess.run(['ffmpeg','-y','-i','/tmp/live.webm','-ar',str(sr),\n",
        "                    '-ac','1','/tmp/live.wav'], capture_output=True)\n",
        "    sr2, audio = wavfile.read('/tmp/live.wav')\n",
        "    return audio.astype(np.float32) / 32768.0, sr2\n",
        "\n",
        "def transcribe_audio(audio, sr):\n",
        "    \"\"\"Run the full pipeline: audio â†’ MFCC â†’ model â†’ text.\"\"\"\n",
        "    # Voice Activity Detection\n",
        "    rms = np.sqrt(np.mean(audio**2))\n",
        "    if rms < 0.015:\n",
        "        return None, rms   # silence\n",
        "\n",
        "    mfcc     = mfcc_ex.extract(audio)\n",
        "    feat     = torch.FloatTensor(mfcc).unsqueeze(0).to(DEVICE)  # (1, T, 13)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_probs = model(feat)                          # (T, 1, C)\n",
        "\n",
        "    preds = log_probs.argmax(dim=2).squeeze(1).cpu().numpy()\n",
        "    text  = vocab.ctc_decode(preds.tolist())\n",
        "    return text, rms\n",
        "\n",
        "# â”€â”€ Stop button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "stop = {'flag': False}\n",
        "def do_stop(): stop['flag'] = True\n",
        "output.register_callback('stop_live', do_stop)\n",
        "\n",
        "# â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "display(HTML(\"\"\"\n",
        "<div style=\"font-family:'Segoe UI',sans-serif;background:#0f172a;color:#e2e8f0;\n",
        "  padding:24px;border-radius:14px;border:1px solid #334155;margin:10px 0;\">\n",
        "  <div style=\"display:flex;align-items:center;gap:14px;margin-bottom:18px;\">\n",
        "    <div style=\"width:12px;height:12px;border-radius:50%;background:#ef4444;\n",
        "      animation:pulse 1.2s infinite;\"></div>\n",
        "    <span style=\"font-size:19px;font-weight:700;\">ğŸ™ï¸ LIVE Sinhala-English Transcription</span>\n",
        "    <button onclick=\"google.colab.kernel.invokeFunction('stop_live',[],{})\"\n",
        "      style=\"margin-left:auto;padding:9px 20px;background:#ef4444;color:white;\n",
        "        border:none;border-radius:8px;cursor:pointer;font-weight:700;font-size:14px;\">\n",
        "      â¹ STOP</button>\n",
        "  </div>\n",
        "  <div id=\"st\" style=\"color:#64748b;font-size:12px;margin-bottom:14px;font-family:monospace;\">\n",
        "    â³ Starting...</div>\n",
        "  <div id=\"lt\" style=\"font-size:22px;font-weight:500;line-height:1.5;color:#f1f5f9;\n",
        "    background:#1e293b;padding:16px;border-radius:10px;border-left:4px solid #3b82f6;\n",
        "    min-height:60px;margin-bottom:14px;\">â€”</div>\n",
        "  <div id=\"ht\" style=\"font-size:13px;color:#94a3b8;max-height:200px;overflow-y:auto;\n",
        "    line-height:2;background:#0f172a;padding:10px 14px;border-radius:8px;\"></div>\n",
        "</div>\n",
        "<style>@keyframes pulse{0%,100%{opacity:1}50%{opacity:0.3}}</style>\n",
        "\"\"\"))\n",
        "\n",
        "def upd(live, hist_html, status):\n",
        "    display(Javascript(f\"\"\"\n",
        "    (function(){{\n",
        "        var a=document.getElementById('lt'),b=document.getElementById('ht'),\n",
        "            c=document.getElementById('st');\n",
        "        if(a) a.innerHTML={json.dumps(live or 'â€”')};\n",
        "        if(b) b.innerHTML={json.dumps(hist_html)};\n",
        "        if(c) c.innerHTML={json.dumps(status)};\n",
        "    }})();\n",
        "    \"\"\"))\n",
        "\n",
        "# â”€â”€ Session loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "history, chunk_n, elapsed = [], 0, 0\n",
        "full_transcript = []\n",
        "\n",
        "print(\"âœ… Starting â€” speak into your microphone!\")\n",
        "print(\"   Click STOP to end the session.\\n\")\n",
        "\n",
        "while not stop['flag']:\n",
        "    chunk_n += 1\n",
        "    elapsed += 5\n",
        "    mm, ss = elapsed // 60, elapsed % 60\n",
        "    upd(\"ğŸ¤ Listening...\", '<br>'.join(history[-12:]),\n",
        "        f\"ğŸ”´ REC  Chunk #{chunk_n}  |  {mm:02d}:{ss:02d}\")\n",
        "\n",
        "    audio, sr = record_chunk(seconds=5)\n",
        "    if audio is None: continue\n",
        "\n",
        "    upd(\"â³ Transcribing...\", '<br>'.join(history[-12:]),\n",
        "        f\"ğŸ”„ Processing chunk #{chunk_n}...\")\n",
        "\n",
        "    text, rms = transcribe_audio(audio, sr)\n",
        "\n",
        "    if text is None:\n",
        "        upd(history[-1].split('  ')[-1] if history else 'â€”',\n",
        "            '<br>'.join(history[-12:]),\n",
        "            f\"ğŸ¤« Silence (energy={rms:.3f})  |  Chunk #{chunk_n}\")\n",
        "        continue\n",
        "\n",
        "    ts  = datetime.now().strftime('%H:%M:%S')\n",
        "    row = f\"<span style='color:#64748b'>[{ts}]</span>  <span style='color:#e2e8f0'>{text}</span>\"\n",
        "    history.append(row)\n",
        "    full_transcript.append(f\"[{ts}]  {text}\")\n",
        "\n",
        "    upd(text, '<br>'.join(history[-12:]),\n",
        "        f\"âœ… Done  |  Chunk #{chunk_n}  |  {mm:02d}:{ss:02d}\")\n",
        "\n",
        "    if stop['flag']: break\n",
        "\n",
        "# â”€â”€ Save transcript â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n\" + \"â”\"*60)\n",
        "print(\"  SESSION COMPLETE\")\n",
        "print(\"â”\"*60)\n",
        "print('\\n'.join(full_transcript))\n",
        "\n",
        "if full_transcript:\n",
        "    ts_file = f\"{DRIVE_DIR}/transcript_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "    with open(ts_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"\\n\".join(full_transcript))\n",
        "    print(f\"\\nğŸ’¾ Transcript saved â†’ {ts_file}\")\n",
        "\n",
        "stop['flag'] = False   # reset for next run"
      ],
      "outputs": [],
      "execution_count": null,
      "id": "iFeg4X1zAoe_"
    }
  ]
}